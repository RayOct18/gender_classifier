import argparse
import better_exceptions
from pathlib import Path
from collections import OrderedDict
from tqdm import tqdm
import numpy as np
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim
from torch.optim.lr_scheduler import StepLR
import torch.utils.data
from torch.utils.data import DataLoader
import torch.nn.functional as F
from logger import Logger
import os

from model.dex_models import Gender
from dataloader import loadImgs
from defaults import _C as cfg


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, default='/media/ray/Ray/database/age/Morph/crop/morhp_whole_256_st/crop_images/crop_images_whole_256', help="Data root directory")
    parser.add_argument('--train_csv', type=str, default='./train_list/morph_age_train.csv', help='csv for training')
    parser.add_argument('--val_csv', type=str, default='./train_list/morph_age_val.csv', help='csv for validation')
    parser.add_argument("--resume", type=str, default=None, help="Resume from checkpoint if any")
    parser.add_argument("--checkpoint", type=str, default="checkpoint", help="Checkpoint directory")
    parser.add_argument("--tensorboard", type=str, default=None, help="Tensorboard log directory")

    args = parser.parse_args()
    return args


class AverageMeter(object):
    def __init__(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val
        self.count += n
        self.avg = self.sum / self.count


def train(train_loader, model, criterion, optimizer, epoch, device):
    model.train()
    loss_monitor = AverageMeter()
    accuracy_monitor = AverageMeter()

    with tqdm(train_loader) as _tqdm:
        for x, y, z in _tqdm:
            x = x.to(device)
            y = y.to(device)

            # denorm
            x = (x + 1) / 2
            x = x.clamp_(0, 1) * 255

            # compute output
            outputs = model(x)

            # calc loss, this criterion don't need softmax before input
            loss = criterion(outputs, y)
            cur_loss = loss.item()

            # calc accuracy
            _, predicted = outputs.max(1)
            correct_num = predicted.eq(y).sum().item()

            # measure accuracy and record loss
            sample_num = x.size(0)
            loss_monitor.update(cur_loss, sample_num)
            accuracy_monitor.update(correct_num, sample_num)

            # compute gradient and do SGD step
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            _tqdm.set_postfix(OrderedDict(stage="train", epoch=epoch, loss=loss_monitor.avg),
                              acc=accuracy_monitor.avg, correct=correct_num, sample_num=sample_num)

    return loss_monitor.avg, accuracy_monitor.avg


def validate(validate_loader, model, criterion, epoch, device):
    model.eval()
    loss_monitor = AverageMeter()
    accuracy_monitor = AverageMeter()
    preds = []
    gt = []

    with torch.no_grad():
        with tqdm(validate_loader) as _tqdm:
            for i, (x, y, z) in enumerate(_tqdm):
                x = x.to(device)
                y = y.to(device)

                # denorm
                x = (x + 1) / 2
                x = x.clamp_(0, 1) * 255

                # compute output
                outputs = model(x)
                preds.append(F.softmax(outputs, dim=-1).cpu().numpy())
                gt.append(y.cpu().numpy())

                # valid for validation, not used for test
                if criterion is not None:
                    # calc loss
                    loss = criterion(outputs, y)
                    cur_loss = loss.item()

                    # calc accuracy
                    _, predicted = outputs.max(1)
                    correct_num = predicted.eq(y).sum().item()

                    # measure accuracy and record loss
                    sample_num = x.size(0)
                    loss_monitor.update(cur_loss, sample_num)
                    accuracy_monitor.update(correct_num, sample_num)
                    _tqdm.set_postfix(OrderedDict(stage="val", epoch=epoch, loss=loss_monitor.avg),
                                      acc=accuracy_monitor.avg, correct=correct_num, sample_num=sample_num)

    preds = np.concatenate(preds, axis=0)

    return loss_monitor.avg, accuracy_monitor.avg


def main():
    args = get_args()

    cfg.freeze()
    start_epoch = 0
    checkpoint_dir = Path(args.checkpoint)
    checkpoint_dir.mkdir(parents=True, exist_ok=True)

    # create model

    model = Gender()
    model.load_state_dict(torch.load('./model/gender_sd.pth'))

    if cfg.TRAIN.OPT == "sgd":
        optimizer = torch.optim.SGD(model.parameters(), lr=cfg.TRAIN.LR,
                                    momentum=cfg.TRAIN.MOMENTUM,
                                    weight_decay=cfg.TRAIN.WEIGHT_DECAY)
    else:
        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.TRAIN.LR)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)

    # optionally resume from a checkpoint
    resume_path = args.resume
    loss = {}

    if resume_path:
        if Path(resume_path).is_file():
            print("=> loading checkpoint '{}'".format(resume_path))
            checkpoint = torch.load(resume_path, map_location="cpu")
            start_epoch = checkpoint['epoch']
            model.load_state_dict(checkpoint['state_dict'])
            print("=> loaded checkpoint '{}' (epoch {})"
                  .format(resume_path, checkpoint['epoch']))
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        else:
            print("=> no checkpoint found at '{}'".format(resume_path))

    if device == "cuda":
        cudnn.benchmark = True

    criterion = nn.CrossEntropyLoss().to(device)
    train_loader = loadImgs(dataset_path=args.data_dir, csv_dir=args.train_csv, img_size=cfg.MODEL.IMG_SIZE, 
                            batchSize=cfg.TRAIN.BATCH_SIZE, num_workers=cfg.TRAIN.WORKERS, shuffle=True, drop_last=True)

    val_loader = loadImgs(dataset_path=args.data_dir, csv_dir=args.val_csv, img_size=cfg.MODEL.IMG_SIZE, 
                            batchSize=cfg.TEST.BATCH_SIZE, num_workers=cfg.TRAIN.WORKERS, shuffle=False, drop_last=False)

    scheduler = StepLR(optimizer, step_size=cfg.TRAIN.LR_DECAY_STEP, gamma=cfg.TRAIN.LR_DECAY_RATE,
                       last_epoch=start_epoch - 1)
    best_val_acc = 0.0
    

    if args.tensorboard is not None:
        tensorboard_dir = args.tensorboard
        if not os.path.exists(tensorboard_dir):
            os.makedirs(tensorboard_dir)
        logger = Logger(tensorboard_dir)

    for epoch in range(start_epoch, cfg.TRAIN.EPOCHS):
        # train
        train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, device)

        # validate
        val_loss, val_acc = validate(val_loader, model, criterion, epoch, device)

        loss['Train/Loss'] = train_loss
        loss['Train/Acc'] = train_acc
        loss['Val/Loss'] = val_loss
        loss['Val/Acc'] = val_acc

        if args.tensorboard is not None:
            for tag, value in loss.items():
                logger.scalar_summary(tag, value, epoch + 1)

        # checkpoint
        if val_acc > best_val_acc or (epoch+1) == cfg.TRAIN.EPOCHS:
            print(f"=> [epoch {epoch:03d}] best val acc was improved from {best_val_acc:.3f} to {val_acc:.3f}")
            model_state_dict = model.state_dict()
            torch.save(
                {
                    'epoch': epoch + 1,
                    'state_dict': model_state_dict,
                    'optimizer_state_dict': optimizer.state_dict()
                },
                str(checkpoint_dir.joinpath("epoch{:03d}_{:.5f}_{:.4f}.pth".format(epoch, val_loss, val_acc)))
            )
            best_val_acc = val_acc
        else:
            print(f"=> [epoch {epoch:03d}] best val acc was not improved from {best_val_acc:.3f} ({val_acc:.3f})")

        # adjust learning rate
        scheduler.step()

    print("=> training finished")
    print(f"best val acc: {best_val_acc:.3f}")


if __name__ == '__main__':
    main()
